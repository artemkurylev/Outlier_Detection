{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AML Project",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/artemkurylev/Outlier_Detection/blob/master/AML_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnwQ7w33Lpek",
        "colab_type": "code",
        "outputId": "1c459a42-f79a-494d-c3f4-6acdb1be66e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "! wget -O data.csv https://www.openml.org/data/get_csv/52214/phpn1jVwe"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-03 08:34:37--  https://www.openml.org/data/get_csv/52214/phpn1jVwe\n",
            "Resolving www.openml.org (www.openml.org)... 131.155.11.11\n",
            "Connecting to www.openml.org (www.openml.org)|131.155.11.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/plain]\n",
            "Saving to: ‘data.csv’\n",
            "\n",
            "data.csv                [   <=>              ] 808.56K  1.50MB/s    in 0.5s    \n",
            "\n",
            "2020-04-03 08:34:38 (1.50 MB/s) - ‘data.csv’ saved [827961]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyGIzPMDMSpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "import torch.nn as nn\n",
        "from torch.functional import F\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.utils import shuffle\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i65USE-vMYyb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"data.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAHZYWL6McYO",
        "colab_type": "code",
        "outputId": "e5785c05-ef5b-4f1b-cbc7-e50eaf67d551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>attr1</th>\n",
              "      <th>attr2</th>\n",
              "      <th>attr3</th>\n",
              "      <th>attr4</th>\n",
              "      <th>attr5</th>\n",
              "      <th>attr6</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.230020</td>\n",
              "      <td>5.072578</td>\n",
              "      <td>-0.276061</td>\n",
              "      <td>0.832444</td>\n",
              "      <td>-0.377866</td>\n",
              "      <td>0.480322</td>\n",
              "      <td>'-1'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.155491</td>\n",
              "      <td>-0.169390</td>\n",
              "      <td>0.670652</td>\n",
              "      <td>-0.859553</td>\n",
              "      <td>-0.377866</td>\n",
              "      <td>-0.945723</td>\n",
              "      <td>'-1'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.784415</td>\n",
              "      <td>-0.443654</td>\n",
              "      <td>5.674705</td>\n",
              "      <td>-0.859553</td>\n",
              "      <td>-0.377866</td>\n",
              "      <td>-0.945723</td>\n",
              "      <td>'-1'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.546088</td>\n",
              "      <td>0.131415</td>\n",
              "      <td>-0.456387</td>\n",
              "      <td>-0.859553</td>\n",
              "      <td>-0.377866</td>\n",
              "      <td>-0.945723</td>\n",
              "      <td>'-1'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.102987</td>\n",
              "      <td>-0.394994</td>\n",
              "      <td>-0.140816</td>\n",
              "      <td>0.979703</td>\n",
              "      <td>-0.377866</td>\n",
              "      <td>1.013566</td>\n",
              "      <td>'-1'</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      attr1     attr2     attr3     attr4     attr5     attr6 class\n",
              "0  0.230020  5.072578 -0.276061  0.832444 -0.377866  0.480322  '-1'\n",
              "1  0.155491 -0.169390  0.670652 -0.859553 -0.377866 -0.945723  '-1'\n",
              "2 -0.784415 -0.443654  5.674705 -0.859553 -0.377866 -0.945723  '-1'\n",
              "3  0.546088  0.131415 -0.456387 -0.859553 -0.377866 -0.945723  '-1'\n",
              "4 -0.102987 -0.394994 -0.140816  0.979703 -0.377866  1.013566  '-1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yz556CipMgej",
        "colab_type": "code",
        "outputId": "d7d6f11e-4210-4199-eca3-d000e364f6fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 11183 entries, 0 to 11182\n",
            "Data columns (total 7 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   attr1   11183 non-null  float64\n",
            " 1   attr2   11183 non-null  float64\n",
            " 2   attr3   11183 non-null  float64\n",
            " 3   attr4   11183 non-null  float64\n",
            " 4   attr5   11183 non-null  float64\n",
            " 5   attr6   11183 non-null  float64\n",
            " 6   class   11183 non-null  object \n",
            "dtypes: float64(6), object(1)\n",
            "memory usage: 611.7+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZWunL3KMjma",
        "colab_type": "code",
        "outputId": "fa31f021-fbc1-4fc8-82d7-65264716f860",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>attr1</th>\n",
              "      <th>attr2</th>\n",
              "      <th>attr3</th>\n",
              "      <th>attr4</th>\n",
              "      <th>attr5</th>\n",
              "      <th>attr6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.118300e+04</td>\n",
              "      <td>1.118300e+04</td>\n",
              "      <td>1.118300e+04</td>\n",
              "      <td>1.118300e+04</td>\n",
              "      <td>1.118300e+04</td>\n",
              "      <td>1.118300e+04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.096535e-10</td>\n",
              "      <td>1.297595e-09</td>\n",
              "      <td>5.698113e-10</td>\n",
              "      <td>-2.435705e-09</td>\n",
              "      <td>-1.120680e-09</td>\n",
              "      <td>1.459483e-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-7.844148e-01</td>\n",
              "      <td>-4.701953e-01</td>\n",
              "      <td>-5.916315e-01</td>\n",
              "      <td>-8.595525e-01</td>\n",
              "      <td>-3.778657e-01</td>\n",
              "      <td>-9.457232e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-7.844148e-01</td>\n",
              "      <td>-4.701953e-01</td>\n",
              "      <td>-5.916315e-01</td>\n",
              "      <td>-8.595525e-01</td>\n",
              "      <td>-3.778657e-01</td>\n",
              "      <td>-9.457232e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-1.085769e-01</td>\n",
              "      <td>-3.949941e-01</td>\n",
              "      <td>-2.309790e-01</td>\n",
              "      <td>-8.595525e-01</td>\n",
              "      <td>-3.778657e-01</td>\n",
              "      <td>-9.457232e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>3.139489e-01</td>\n",
              "      <td>-7.649473e-02</td>\n",
              "      <td>2.198366e-01</td>\n",
              "      <td>8.202077e-01</td>\n",
              "      <td>-3.778657e-01</td>\n",
              "      <td>1.016613e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>3.150844e+01</td>\n",
              "      <td>5.085849e+00</td>\n",
              "      <td>2.947777e+01</td>\n",
              "      <td>9.591164e+00</td>\n",
              "      <td>2.361712e+01</td>\n",
              "      <td>1.949027e+00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              attr1         attr2  ...         attr5         attr6\n",
              "count  1.118300e+04  1.118300e+04  ...  1.118300e+04  1.118300e+04\n",
              "mean   1.096535e-10  1.297595e-09  ... -1.120680e-09  1.459483e-09\n",
              "std    1.000000e+00  1.000000e+00  ...  1.000000e+00  1.000000e+00\n",
              "min   -7.844148e-01 -4.701953e-01  ... -3.778657e-01 -9.457232e-01\n",
              "25%   -7.844148e-01 -4.701953e-01  ... -3.778657e-01 -9.457232e-01\n",
              "50%   -1.085769e-01 -3.949941e-01  ... -3.778657e-01 -9.457232e-01\n",
              "75%    3.139489e-01 -7.649473e-02  ... -3.778657e-01  1.016613e+00\n",
              "max    3.150844e+01  5.085849e+00  ...  2.361712e+01  1.949027e+00\n",
              "\n",
              "[8 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmggsBATMnw_",
        "colab_type": "code",
        "outputId": "47df1e14-9429-4a24-91d8-cbf5afadae7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "source": [
        "df['class'].hist()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f88cdae5748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEACAYAAAB/BTv2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDBdJUnGGiySpOMNFklTc/wfDshLOhuhr+wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcfYNQeKNOuc",
        "colab_type": "code",
        "outputId": "0a5c45fc-dd0a-49c9-b88e-506c98ff1a82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df['target'] = df['class'].apply(lambda x : 0 if x == \"'-1'\" else 1)\n",
        "df.head()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>attr1</th>\n",
              "      <th>attr2</th>\n",
              "      <th>attr3</th>\n",
              "      <th>attr4</th>\n",
              "      <th>attr5</th>\n",
              "      <th>attr6</th>\n",
              "      <th>class</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.230020</td>\n",
              "      <td>5.072578</td>\n",
              "      <td>-0.276061</td>\n",
              "      <td>0.832444</td>\n",
              "      <td>-0.377866</td>\n",
              "      <td>0.480322</td>\n",
              "      <td>'-1'</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.155491</td>\n",
              "      <td>-0.169390</td>\n",
              "      <td>0.670652</td>\n",
              "      <td>-0.859553</td>\n",
              "      <td>-0.377866</td>\n",
              "      <td>-0.945723</td>\n",
              "      <td>'-1'</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.784415</td>\n",
              "      <td>-0.443654</td>\n",
              "      <td>5.674705</td>\n",
              "      <td>-0.859553</td>\n",
              "      <td>-0.377866</td>\n",
              "      <td>-0.945723</td>\n",
              "      <td>'-1'</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.546088</td>\n",
              "      <td>0.131415</td>\n",
              "      <td>-0.456387</td>\n",
              "      <td>-0.859553</td>\n",
              "      <td>-0.377866</td>\n",
              "      <td>-0.945723</td>\n",
              "      <td>'-1'</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.102987</td>\n",
              "      <td>-0.394994</td>\n",
              "      <td>-0.140816</td>\n",
              "      <td>0.979703</td>\n",
              "      <td>-0.377866</td>\n",
              "      <td>1.013566</td>\n",
              "      <td>'-1'</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      attr1     attr2     attr3     attr4     attr5     attr6 class  target\n",
              "0  0.230020  5.072578 -0.276061  0.832444 -0.377866  0.480322  '-1'       0\n",
              "1  0.155491 -0.169390  0.670652 -0.859553 -0.377866 -0.945723  '-1'       0\n",
              "2 -0.784415 -0.443654  5.674705 -0.859553 -0.377866 -0.945723  '-1'       0\n",
              "3  0.546088  0.131415 -0.456387 -0.859553 -0.377866 -0.945723  '-1'       0\n",
              "4 -0.102987 -0.394994 -0.140816  0.979703 -0.377866  1.013566  '-1'       0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG3lv0y6UJNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df[['attr1','attr2','attr3','attr4','attr5','attr6']]\n",
        "Y = df[['target']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHvPjEPC0BlR",
        "colab_type": "code",
        "outputId": "4f1b8d8c-705a-4179-c276-fb0d40c001fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Y['target'].unique()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-q3D6v-V4uAM",
        "colab_type": "code",
        "outputId": "a71dfd2a-1ff2-4ed4-8e0d-bfe2ea03d223",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "source": [
        "Y['target'].hist()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f88cd9955c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEACAYAAAB/BTv2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8xTBxAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXxEq4mL4LxG",
        "colab_type": "code",
        "outputId": "edd491fa-2df4-410c-961e-72a08b090876",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from collections import Counter\n",
        "Counter(Y['target'])"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 10923, 1: 260})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e92YQmEZOtsu",
        "colab_type": "text"
      },
      "source": [
        "#### Normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WST8KoCwOvtX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = X.values #returns a numpy array\n",
        "min_max_scaler = MinMaxScaler()\n",
        "x_scaled = min_max_scaler.fit_transform(x)\n",
        "X = pd.DataFrame(x_scaled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJvN_nc6Tum0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLfCzx2HM-tC",
        "colab_type": "text"
      },
      "source": [
        "#### Oversampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YA8wEIzNB4y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "de0024fb-eca0-425e-c124-2ab492659b71"
      },
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "ros = RandomOverSampler(random_state=0)\n",
        "X_resampled, y_resampled = ros.fit_resample(X_train, y_train.to_numpy().ravel())\n",
        "X_resampled, y_resampled = shuffle(X_resampled,y_resampled)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U39CPWI8R3ku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = TensorDataset(torch.Tensor(X_resampled), torch.Tensor(y_resampled.reshape(y_resampled.shape[0],-1)))\n",
        "test_dataset = TensorDataset(torch.Tensor(X_test.to_numpy()), torch.Tensor(y_test.to_numpy()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UeeH5dFUzey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otvl8oViUiHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_loader = torch.utils.data.DataLoader(train_dataset,batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yem4BPXU-ly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Baseline(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Baseline,self).__init__()\n",
        "    self.fc1 = nn.Sequential(\n",
        "        nn.Linear(6,128),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.Linear(128,1024),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.Linear(1024,1024),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.Linear(1024,1)\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    out = self.fc1(x)\n",
        "    #out = torch.sigmoid(out)\n",
        "    \n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBTW0YS_I1Nq",
        "colab_type": "text"
      },
      "source": [
        "#### Cuda"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtZlkOnYWeoy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cuda = True if torch.cuda.is_available() else False\n",
        "device = 'cuda' if cuda else 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggVYd8YEIvY-",
        "colab_type": "text"
      },
      "source": [
        "#### Weight initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIe9GkxtIs7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMnMY20tWmZD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "11df4a35-f2de-4515-e2e6-73ea4c95f32e"
      },
      "source": [
        "baseline = Baseline()\n",
        "if cuda:\n",
        "  baseline.cuda()\n",
        "baseline.fc1.apply(init_weights)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=6, out_features=128, bias=True)\n",
              "  (1): LeakyReLU(negative_slope=0.01)\n",
              "  (2): Linear(in_features=128, out_features=1024, bias=True)\n",
              "  (3): LeakyReLU(negative_slope=0.01)\n",
              "  (4): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "  (5): LeakyReLU(negative_slope=0.01)\n",
              "  (6): Linear(in_features=1024, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oodLXuStW5bj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.01\n",
        "optimizer = optim.Adam(baseline.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odH1lfGWXLQX",
        "colab_type": "code",
        "outputId": "c1841496-985e-4c3b-ce21-d5afc5ea28e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "n_epochs = 25\n",
        "n_print = 100\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  total, correct = 0, 0\n",
        "  baseline.train()\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  for i, (data,target) in enumerate(data_loader):\n",
        "    optimizer.zero_grad()\n",
        "    output = baseline(data.to(device))\n",
        "    loss =  criterion(output.float(),target.to(device))\n",
        "    loss.backward()\n",
        "\n",
        "    # Compute train accuracy\n",
        "    _, predicted = torch.max(output.data, 1)\n",
        "    batch_total = target.size(0)\n",
        "    total += batch_total\n",
        "    #print(output.data == target.data)\n",
        "    batch_correct = (torch.round(torch.sigmoid(output.data)) == target.to(device).data).sum()\n",
        "    correct += batch_correct\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i + 1) % n_print == 0:\n",
        "      print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
        "            % (epoch + 1, n_epochs, i + 1, len(train_dataset) // batch_size, loss.item()))\n",
        "  print(\"correct: {}\\ntotal: {}\".format(correct, total))\n",
        "  print(\"Accuracy on Epoch[{}/{}]: {}%\".format(epoch + 1, n_epochs, 100* correct/total ))\n"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/25], Step [100/1747], Loss: 0.5848\n",
            "Epoch [1/25], Step [200/1747], Loss: 0.0336\n",
            "Epoch [1/25], Step [300/1747], Loss: 0.3155\n",
            "Epoch [1/25], Step [400/1747], Loss: 0.1086\n",
            "Epoch [1/25], Step [500/1747], Loss: 0.0352\n",
            "Epoch [1/25], Step [600/1747], Loss: 0.0598\n",
            "Epoch [1/25], Step [700/1747], Loss: 0.0848\n",
            "Epoch [1/25], Step [800/1747], Loss: 0.0661\n",
            "Epoch [1/25], Step [900/1747], Loss: 0.0522\n",
            "Epoch [1/25], Step [1000/1747], Loss: 0.5603\n",
            "Epoch [1/25], Step [1100/1747], Loss: 0.0274\n",
            "Epoch [1/25], Step [1200/1747], Loss: 0.3574\n",
            "Epoch [1/25], Step [1300/1747], Loss: 0.1292\n",
            "Epoch [1/25], Step [1400/1747], Loss: 0.1849\n",
            "Epoch [1/25], Step [1500/1747], Loss: 0.4459\n",
            "Epoch [1/25], Step [1600/1747], Loss: 0.0638\n",
            "Epoch [1/25], Step [1700/1747], Loss: 0.2908\n",
            "correct: 15692\n",
            "total: 17476\n",
            "Accuracy on Epoch[1/25]: 89%\n",
            "Epoch [2/25], Step [100/1747], Loss: 0.6368\n",
            "Epoch [2/25], Step [200/1747], Loss: 0.0497\n",
            "Epoch [2/25], Step [300/1747], Loss: 0.2981\n",
            "Epoch [2/25], Step [400/1747], Loss: 0.1466\n",
            "Epoch [2/25], Step [500/1747], Loss: 0.0204\n",
            "Epoch [2/25], Step [600/1747], Loss: 0.0788\n",
            "Epoch [2/25], Step [700/1747], Loss: 0.0644\n",
            "Epoch [2/25], Step [800/1747], Loss: 0.0724\n",
            "Epoch [2/25], Step [900/1747], Loss: 0.0634\n",
            "Epoch [2/25], Step [1000/1747], Loss: 0.5376\n",
            "Epoch [2/25], Step [1100/1747], Loss: 0.0456\n",
            "Epoch [2/25], Step [1200/1747], Loss: 0.4017\n",
            "Epoch [2/25], Step [1300/1747], Loss: 0.1353\n",
            "Epoch [2/25], Step [1400/1747], Loss: 0.1372\n",
            "Epoch [2/25], Step [1500/1747], Loss: 0.4611\n",
            "Epoch [2/25], Step [1600/1747], Loss: 0.0569\n",
            "Epoch [2/25], Step [1700/1747], Loss: 0.5427\n",
            "correct: 15690\n",
            "total: 17476\n",
            "Accuracy on Epoch[2/25]: 89%\n",
            "Epoch [3/25], Step [100/1747], Loss: 0.4887\n",
            "Epoch [3/25], Step [200/1747], Loss: 0.0631\n",
            "Epoch [3/25], Step [300/1747], Loss: 0.2061\n",
            "Epoch [3/25], Step [400/1747], Loss: 0.1156\n",
            "Epoch [3/25], Step [500/1747], Loss: 0.0341\n",
            "Epoch [3/25], Step [600/1747], Loss: 0.0359\n",
            "Epoch [3/25], Step [700/1747], Loss: 0.0794\n",
            "Epoch [3/25], Step [800/1747], Loss: 0.0466\n",
            "Epoch [3/25], Step [900/1747], Loss: 0.0486\n",
            "Epoch [3/25], Step [1000/1747], Loss: 0.4956\n",
            "Epoch [3/25], Step [1100/1747], Loss: 0.2773\n",
            "Epoch [3/25], Step [1200/1747], Loss: 0.3690\n",
            "Epoch [3/25], Step [1300/1747], Loss: 0.2587\n",
            "Epoch [3/25], Step [1400/1747], Loss: 0.1168\n",
            "Epoch [3/25], Step [1500/1747], Loss: 0.4711\n",
            "Epoch [3/25], Step [1600/1747], Loss: 0.0423\n",
            "Epoch [3/25], Step [1700/1747], Loss: 0.5057\n",
            "correct: 15577\n",
            "total: 17476\n",
            "Accuracy on Epoch[3/25]: 89%\n",
            "Epoch [4/25], Step [100/1747], Loss: 0.6309\n",
            "Epoch [4/25], Step [200/1747], Loss: 0.0248\n",
            "Epoch [4/25], Step [300/1747], Loss: 0.1919\n",
            "Epoch [4/25], Step [400/1747], Loss: 0.1570\n",
            "Epoch [4/25], Step [500/1747], Loss: 0.0344\n",
            "Epoch [4/25], Step [600/1747], Loss: 0.0865\n",
            "Epoch [4/25], Step [700/1747], Loss: 0.0463\n",
            "Epoch [4/25], Step [800/1747], Loss: 0.0764\n",
            "Epoch [4/25], Step [900/1747], Loss: 0.0763\n",
            "Epoch [4/25], Step [1000/1747], Loss: 0.5122\n",
            "Epoch [4/25], Step [1100/1747], Loss: 0.0273\n",
            "Epoch [4/25], Step [1200/1747], Loss: 0.2664\n",
            "Epoch [4/25], Step [1300/1747], Loss: 0.0963\n",
            "Epoch [4/25], Step [1400/1747], Loss: 0.0919\n",
            "Epoch [4/25], Step [1500/1747], Loss: 0.5328\n",
            "Epoch [4/25], Step [1600/1747], Loss: 0.0525\n",
            "Epoch [4/25], Step [1700/1747], Loss: 0.2298\n",
            "correct: 15692\n",
            "total: 17476\n",
            "Accuracy on Epoch[4/25]: 89%\n",
            "Epoch [5/25], Step [100/1747], Loss: 0.6994\n",
            "Epoch [5/25], Step [200/1747], Loss: 0.0243\n",
            "Epoch [5/25], Step [300/1747], Loss: 0.2577\n",
            "Epoch [5/25], Step [400/1747], Loss: 0.2152\n",
            "Epoch [5/25], Step [500/1747], Loss: 0.0179\n",
            "Epoch [5/25], Step [600/1747], Loss: 0.0597\n",
            "Epoch [5/25], Step [700/1747], Loss: 0.0675\n",
            "Epoch [5/25], Step [800/1747], Loss: 0.0299\n",
            "Epoch [5/25], Step [900/1747], Loss: 0.0341\n",
            "Epoch [5/25], Step [1000/1747], Loss: 0.5785\n",
            "Epoch [5/25], Step [1100/1747], Loss: 0.0355\n",
            "Epoch [5/25], Step [1200/1747], Loss: 0.3573\n",
            "Epoch [5/25], Step [1300/1747], Loss: 0.0875\n",
            "Epoch [5/25], Step [1400/1747], Loss: 0.1237\n",
            "Epoch [5/25], Step [1500/1747], Loss: 0.4900\n",
            "Epoch [5/25], Step [1600/1747], Loss: 0.0309\n",
            "Epoch [5/25], Step [1700/1747], Loss: 0.6215\n",
            "correct: 15633\n",
            "total: 17476\n",
            "Accuracy on Epoch[5/25]: 89%\n",
            "Epoch [6/25], Step [100/1747], Loss: 0.5760\n",
            "Epoch [6/25], Step [200/1747], Loss: 0.0197\n",
            "Epoch [6/25], Step [300/1747], Loss: 0.2037\n",
            "Epoch [6/25], Step [400/1747], Loss: 0.1366\n",
            "Epoch [6/25], Step [500/1747], Loss: 0.0213\n",
            "Epoch [6/25], Step [600/1747], Loss: 0.0347\n",
            "Epoch [6/25], Step [700/1747], Loss: 0.0717\n",
            "Epoch [6/25], Step [800/1747], Loss: 0.0579\n",
            "Epoch [6/25], Step [900/1747], Loss: 0.0707\n",
            "Epoch [6/25], Step [1000/1747], Loss: 0.5282\n",
            "Epoch [6/25], Step [1100/1747], Loss: 0.0189\n",
            "Epoch [6/25], Step [1200/1747], Loss: 0.2883\n",
            "Epoch [6/25], Step [1300/1747], Loss: 0.1620\n",
            "Epoch [6/25], Step [1400/1747], Loss: 0.0897\n",
            "Epoch [6/25], Step [1500/1747], Loss: 0.5088\n",
            "Epoch [6/25], Step [1600/1747], Loss: 0.0494\n",
            "Epoch [6/25], Step [1700/1747], Loss: 0.5376\n",
            "correct: 15701\n",
            "total: 17476\n",
            "Accuracy on Epoch[6/25]: 89%\n",
            "Epoch [7/25], Step [100/1747], Loss: 0.5078\n",
            "Epoch [7/25], Step [200/1747], Loss: 0.0204\n",
            "Epoch [7/25], Step [300/1747], Loss: 0.2106\n",
            "Epoch [7/25], Step [400/1747], Loss: 0.1526\n",
            "Epoch [7/25], Step [500/1747], Loss: 0.0283\n",
            "Epoch [7/25], Step [600/1747], Loss: 0.0285\n",
            "Epoch [7/25], Step [700/1747], Loss: 0.0807\n",
            "Epoch [7/25], Step [800/1747], Loss: 0.0809\n",
            "Epoch [7/25], Step [900/1747], Loss: 0.0480\n",
            "Epoch [7/25], Step [1000/1747], Loss: 0.6130\n",
            "Epoch [7/25], Step [1100/1747], Loss: 0.0084\n",
            "Epoch [7/25], Step [1200/1747], Loss: 0.2973\n",
            "Epoch [7/25], Step [1300/1747], Loss: 0.0952\n",
            "Epoch [7/25], Step [1400/1747], Loss: 0.1506\n",
            "Epoch [7/25], Step [1500/1747], Loss: 0.6754\n",
            "Epoch [7/25], Step [1600/1747], Loss: 0.0494\n",
            "Epoch [7/25], Step [1700/1747], Loss: 0.4820\n",
            "correct: 15661\n",
            "total: 17476\n",
            "Accuracy on Epoch[7/25]: 89%\n",
            "Epoch [8/25], Step [100/1747], Loss: 0.7719\n",
            "Epoch [8/25], Step [200/1747], Loss: 0.0656\n",
            "Epoch [8/25], Step [300/1747], Loss: 0.2185\n",
            "Epoch [8/25], Step [400/1747], Loss: 0.1704\n",
            "Epoch [8/25], Step [500/1747], Loss: 0.0215\n",
            "Epoch [8/25], Step [600/1747], Loss: 0.0613\n",
            "Epoch [8/25], Step [700/1747], Loss: 0.0828\n",
            "Epoch [8/25], Step [800/1747], Loss: 0.0489\n",
            "Epoch [8/25], Step [900/1747], Loss: 0.0367\n",
            "Epoch [8/25], Step [1000/1747], Loss: 0.5506\n",
            "Epoch [8/25], Step [1100/1747], Loss: 0.0056\n",
            "Epoch [8/25], Step [1200/1747], Loss: 0.2671\n",
            "Epoch [8/25], Step [1300/1747], Loss: 0.1050\n",
            "Epoch [8/25], Step [1400/1747], Loss: 0.1076\n",
            "Epoch [8/25], Step [1500/1747], Loss: 0.3464\n",
            "Epoch [8/25], Step [1600/1747], Loss: 0.0470\n",
            "Epoch [8/25], Step [1700/1747], Loss: 0.5363\n",
            "correct: 15671\n",
            "total: 17476\n",
            "Accuracy on Epoch[8/25]: 89%\n",
            "Epoch [9/25], Step [100/1747], Loss: 0.5301\n",
            "Epoch [9/25], Step [200/1747], Loss: 0.0271\n",
            "Epoch [9/25], Step [300/1747], Loss: 0.2661\n",
            "Epoch [9/25], Step [400/1747], Loss: 0.1709\n",
            "Epoch [9/25], Step [500/1747], Loss: 0.0188\n",
            "Epoch [9/25], Step [600/1747], Loss: 0.0532\n",
            "Epoch [9/25], Step [700/1747], Loss: 0.0924\n",
            "Epoch [9/25], Step [800/1747], Loss: 0.0756\n",
            "Epoch [9/25], Step [900/1747], Loss: 0.0985\n",
            "Epoch [9/25], Step [1000/1747], Loss: 0.5514\n",
            "Epoch [9/25], Step [1100/1747], Loss: 0.0035\n",
            "Epoch [9/25], Step [1200/1747], Loss: 0.2825\n",
            "Epoch [9/25], Step [1300/1747], Loss: 0.1147\n",
            "Epoch [9/25], Step [1400/1747], Loss: 0.1736\n",
            "Epoch [9/25], Step [1500/1747], Loss: 0.4078\n",
            "Epoch [9/25], Step [1600/1747], Loss: 0.0346\n",
            "Epoch [9/25], Step [1700/1747], Loss: 0.5672\n",
            "correct: 15740\n",
            "total: 17476\n",
            "Accuracy on Epoch[9/25]: 90%\n",
            "Epoch [10/25], Step [100/1747], Loss: 0.4810\n",
            "Epoch [10/25], Step [200/1747], Loss: 0.0558\n",
            "Epoch [10/25], Step [300/1747], Loss: 0.1872\n",
            "Epoch [10/25], Step [400/1747], Loss: 0.1134\n",
            "Epoch [10/25], Step [500/1747], Loss: 0.0196\n",
            "Epoch [10/25], Step [600/1747], Loss: 0.0625\n",
            "Epoch [10/25], Step [700/1747], Loss: 0.0853\n",
            "Epoch [10/25], Step [800/1747], Loss: 0.0753\n",
            "Epoch [10/25], Step [900/1747], Loss: 0.0373\n",
            "Epoch [10/25], Step [1000/1747], Loss: 0.4788\n",
            "Epoch [10/25], Step [1100/1747], Loss: 0.0042\n",
            "Epoch [10/25], Step [1200/1747], Loss: 0.3063\n",
            "Epoch [10/25], Step [1300/1747], Loss: 0.1156\n",
            "Epoch [10/25], Step [1400/1747], Loss: 0.1680\n",
            "Epoch [10/25], Step [1500/1747], Loss: 0.4404\n",
            "Epoch [10/25], Step [1600/1747], Loss: 0.0379\n",
            "Epoch [10/25], Step [1700/1747], Loss: 0.3121\n",
            "correct: 15774\n",
            "total: 17476\n",
            "Accuracy on Epoch[10/25]: 90%\n",
            "Epoch [11/25], Step [100/1747], Loss: 0.5625\n",
            "Epoch [11/25], Step [200/1747], Loss: 0.0601\n",
            "Epoch [11/25], Step [300/1747], Loss: 0.2134\n",
            "Epoch [11/25], Step [400/1747], Loss: 0.1573\n",
            "Epoch [11/25], Step [500/1747], Loss: 0.0318\n",
            "Epoch [11/25], Step [600/1747], Loss: 0.0391\n",
            "Epoch [11/25], Step [700/1747], Loss: 0.0807\n",
            "Epoch [11/25], Step [800/1747], Loss: 0.0880\n",
            "Epoch [11/25], Step [900/1747], Loss: 0.0775\n",
            "Epoch [11/25], Step [1000/1747], Loss: 0.5092\n",
            "Epoch [11/25], Step [1100/1747], Loss: 0.0192\n",
            "Epoch [11/25], Step [1200/1747], Loss: 0.3116\n",
            "Epoch [11/25], Step [1300/1747], Loss: 0.1273\n",
            "Epoch [11/25], Step [1400/1747], Loss: 0.2320\n",
            "Epoch [11/25], Step [1500/1747], Loss: 0.3260\n",
            "Epoch [11/25], Step [1600/1747], Loss: 0.0345\n",
            "Epoch [11/25], Step [1700/1747], Loss: 0.4798\n",
            "correct: 15697\n",
            "total: 17476\n",
            "Accuracy on Epoch[11/25]: 89%\n",
            "Epoch [12/25], Step [100/1747], Loss: 0.5018\n",
            "Epoch [12/25], Step [200/1747], Loss: 0.0258\n",
            "Epoch [12/25], Step [300/1747], Loss: 0.2025\n",
            "Epoch [12/25], Step [400/1747], Loss: 0.1740\n",
            "Epoch [12/25], Step [500/1747], Loss: 0.0323\n",
            "Epoch [12/25], Step [600/1747], Loss: 0.0405\n",
            "Epoch [12/25], Step [700/1747], Loss: 0.0674\n",
            "Epoch [12/25], Step [800/1747], Loss: 0.0309\n",
            "Epoch [12/25], Step [900/1747], Loss: 0.0518\n",
            "Epoch [12/25], Step [1000/1747], Loss: 0.8860\n",
            "Epoch [12/25], Step [1100/1747], Loss: 0.0387\n",
            "Epoch [12/25], Step [1200/1747], Loss: 0.3547\n",
            "Epoch [12/25], Step [1300/1747], Loss: 0.0975\n",
            "Epoch [12/25], Step [1400/1747], Loss: 0.1025\n",
            "Epoch [12/25], Step [1500/1747], Loss: 0.3629\n",
            "Epoch [12/25], Step [1600/1747], Loss: 0.0297\n",
            "Epoch [12/25], Step [1700/1747], Loss: 0.4448\n",
            "correct: 15592\n",
            "total: 17476\n",
            "Accuracy on Epoch[12/25]: 89%\n",
            "Epoch [13/25], Step [100/1747], Loss: 0.5072\n",
            "Epoch [13/25], Step [200/1747], Loss: 0.0277\n",
            "Epoch [13/25], Step [300/1747], Loss: 0.3703\n",
            "Epoch [13/25], Step [400/1747], Loss: 0.1362\n",
            "Epoch [13/25], Step [500/1747], Loss: 0.0220\n",
            "Epoch [13/25], Step [600/1747], Loss: 0.0359\n",
            "Epoch [13/25], Step [700/1747], Loss: 0.0704\n",
            "Epoch [13/25], Step [800/1747], Loss: 0.0734\n",
            "Epoch [13/25], Step [900/1747], Loss: 0.0895\n",
            "Epoch [13/25], Step [1000/1747], Loss: 0.4874\n",
            "Epoch [13/25], Step [1100/1747], Loss: 0.0434\n",
            "Epoch [13/25], Step [1200/1747], Loss: 0.3192\n",
            "Epoch [13/25], Step [1300/1747], Loss: 0.1021\n",
            "Epoch [13/25], Step [1400/1747], Loss: 0.1063\n",
            "Epoch [13/25], Step [1500/1747], Loss: 0.5800\n",
            "Epoch [13/25], Step [1600/1747], Loss: 0.0307\n",
            "Epoch [13/25], Step [1700/1747], Loss: 0.4782\n",
            "correct: 15751\n",
            "total: 17476\n",
            "Accuracy on Epoch[13/25]: 90%\n",
            "Epoch [14/25], Step [100/1747], Loss: 0.6543\n",
            "Epoch [14/25], Step [200/1747], Loss: 0.0449\n",
            "Epoch [14/25], Step [300/1747], Loss: 0.2367\n",
            "Epoch [14/25], Step [400/1747], Loss: 0.1550\n",
            "Epoch [14/25], Step [500/1747], Loss: 0.0240\n",
            "Epoch [14/25], Step [600/1747], Loss: 0.1521\n",
            "Epoch [14/25], Step [700/1747], Loss: 0.0509\n",
            "Epoch [14/25], Step [800/1747], Loss: 0.0566\n",
            "Epoch [14/25], Step [900/1747], Loss: 0.0922\n",
            "Epoch [14/25], Step [1000/1747], Loss: 0.5028\n",
            "Epoch [14/25], Step [1100/1747], Loss: 0.0383\n",
            "Epoch [14/25], Step [1200/1747], Loss: 0.3293\n",
            "Epoch [14/25], Step [1300/1747], Loss: 0.1165\n",
            "Epoch [14/25], Step [1400/1747], Loss: 0.0890\n",
            "Epoch [14/25], Step [1500/1747], Loss: 0.3009\n",
            "Epoch [14/25], Step [1600/1747], Loss: 0.0339\n",
            "Epoch [14/25], Step [1700/1747], Loss: 0.5164\n",
            "correct: 15753\n",
            "total: 17476\n",
            "Accuracy on Epoch[14/25]: 90%\n",
            "Epoch [15/25], Step [100/1747], Loss: 0.5069\n",
            "Epoch [15/25], Step [200/1747], Loss: 0.0161\n",
            "Epoch [15/25], Step [300/1747], Loss: 0.1825\n",
            "Epoch [15/25], Step [400/1747], Loss: 0.1431\n",
            "Epoch [15/25], Step [500/1747], Loss: 0.0200\n",
            "Epoch [15/25], Step [600/1747], Loss: 0.0258\n",
            "Epoch [15/25], Step [700/1747], Loss: 0.0739\n",
            "Epoch [15/25], Step [800/1747], Loss: 0.0769\n",
            "Epoch [15/25], Step [900/1747], Loss: 0.0848\n",
            "Epoch [15/25], Step [1000/1747], Loss: 0.5532\n",
            "Epoch [15/25], Step [1100/1747], Loss: 0.0210\n",
            "Epoch [15/25], Step [1200/1747], Loss: 0.2170\n",
            "Epoch [15/25], Step [1300/1747], Loss: 0.1083\n",
            "Epoch [15/25], Step [1400/1747], Loss: 0.1400\n",
            "Epoch [15/25], Step [1500/1747], Loss: 0.3141\n",
            "Epoch [15/25], Step [1600/1747], Loss: 0.0331\n",
            "Epoch [15/25], Step [1700/1747], Loss: 0.4773\n",
            "correct: 15807\n",
            "total: 17476\n",
            "Accuracy on Epoch[15/25]: 90%\n",
            "Epoch [16/25], Step [100/1747], Loss: 0.5970\n",
            "Epoch [16/25], Step [200/1747], Loss: 0.0232\n",
            "Epoch [16/25], Step [300/1747], Loss: 0.2048\n",
            "Epoch [16/25], Step [400/1747], Loss: 0.1591\n",
            "Epoch [16/25], Step [500/1747], Loss: 0.0187\n",
            "Epoch [16/25], Step [600/1747], Loss: 0.0562\n",
            "Epoch [16/25], Step [700/1747], Loss: 0.0706\n",
            "Epoch [16/25], Step [800/1747], Loss: 0.0816\n",
            "Epoch [16/25], Step [900/1747], Loss: 0.0671\n",
            "Epoch [16/25], Step [1000/1747], Loss: 0.6044\n",
            "Epoch [16/25], Step [1100/1747], Loss: 0.0589\n",
            "Epoch [16/25], Step [1200/1747], Loss: 0.2806\n",
            "Epoch [16/25], Step [1300/1747], Loss: 0.1558\n",
            "Epoch [16/25], Step [1400/1747], Loss: 0.1009\n",
            "Epoch [16/25], Step [1500/1747], Loss: 0.3871\n",
            "Epoch [16/25], Step [1600/1747], Loss: 0.0294\n",
            "Epoch [16/25], Step [1700/1747], Loss: 0.1483\n",
            "correct: 15812\n",
            "total: 17476\n",
            "Accuracy on Epoch[16/25]: 90%\n",
            "Epoch [17/25], Step [100/1747], Loss: 0.6359\n",
            "Epoch [17/25], Step [200/1747], Loss: 0.0457\n",
            "Epoch [17/25], Step [300/1747], Loss: 0.2690\n",
            "Epoch [17/25], Step [400/1747], Loss: 0.1639\n",
            "Epoch [17/25], Step [500/1747], Loss: 0.0198\n",
            "Epoch [17/25], Step [600/1747], Loss: 0.0605\n",
            "Epoch [17/25], Step [700/1747], Loss: 0.0745\n",
            "Epoch [17/25], Step [800/1747], Loss: 0.0631\n",
            "Epoch [17/25], Step [900/1747], Loss: 0.0248\n",
            "Epoch [17/25], Step [1000/1747], Loss: 0.5360\n",
            "Epoch [17/25], Step [1100/1747], Loss: 0.0307\n",
            "Epoch [17/25], Step [1200/1747], Loss: 0.2867\n",
            "Epoch [17/25], Step [1300/1747], Loss: 0.1019\n",
            "Epoch [17/25], Step [1400/1747], Loss: 0.1237\n",
            "Epoch [17/25], Step [1500/1747], Loss: 0.4041\n",
            "Epoch [17/25], Step [1600/1747], Loss: 0.0384\n",
            "Epoch [17/25], Step [1700/1747], Loss: 0.3627\n",
            "correct: 15768\n",
            "total: 17476\n",
            "Accuracy on Epoch[17/25]: 90%\n",
            "Epoch [18/25], Step [100/1747], Loss: 0.5806\n",
            "Epoch [18/25], Step [200/1747], Loss: 1.0757\n",
            "Epoch [18/25], Step [300/1747], Loss: 0.2602\n",
            "Epoch [18/25], Step [400/1747], Loss: 0.1713\n",
            "Epoch [18/25], Step [500/1747], Loss: 0.0270\n",
            "Epoch [18/25], Step [600/1747], Loss: 0.0546\n",
            "Epoch [18/25], Step [700/1747], Loss: 0.0877\n",
            "Epoch [18/25], Step [800/1747], Loss: 0.0589\n",
            "Epoch [18/25], Step [900/1747], Loss: 0.0713\n",
            "Epoch [18/25], Step [1000/1747], Loss: 0.5523\n",
            "Epoch [18/25], Step [1100/1747], Loss: 0.0559\n",
            "Epoch [18/25], Step [1200/1747], Loss: 0.5384\n",
            "Epoch [18/25], Step [1300/1747], Loss: 0.1025\n",
            "Epoch [18/25], Step [1400/1747], Loss: 0.0975\n",
            "Epoch [18/25], Step [1500/1747], Loss: 0.4163\n",
            "Epoch [18/25], Step [1600/1747], Loss: 0.0685\n",
            "Epoch [18/25], Step [1700/1747], Loss: 0.4109\n",
            "correct: 15576\n",
            "total: 17476\n",
            "Accuracy on Epoch[18/25]: 89%\n",
            "Epoch [19/25], Step [100/1747], Loss: 0.7799\n",
            "Epoch [19/25], Step [200/1747], Loss: 0.0319\n",
            "Epoch [19/25], Step [300/1747], Loss: 0.2381\n",
            "Epoch [19/25], Step [400/1747], Loss: 0.2206\n",
            "Epoch [19/25], Step [500/1747], Loss: 0.0252\n",
            "Epoch [19/25], Step [600/1747], Loss: 0.0323\n",
            "Epoch [19/25], Step [700/1747], Loss: 0.0668\n",
            "Epoch [19/25], Step [800/1747], Loss: 0.0647\n",
            "Epoch [19/25], Step [900/1747], Loss: 0.0844\n",
            "Epoch [19/25], Step [1000/1747], Loss: 0.6422\n",
            "Epoch [19/25], Step [1100/1747], Loss: 0.0086\n",
            "Epoch [19/25], Step [1200/1747], Loss: 0.3415\n",
            "Epoch [19/25], Step [1300/1747], Loss: 0.1639\n",
            "Epoch [19/25], Step [1400/1747], Loss: 0.0601\n",
            "Epoch [19/25], Step [1500/1747], Loss: 0.3592\n",
            "Epoch [19/25], Step [1600/1747], Loss: 0.0221\n",
            "Epoch [19/25], Step [1700/1747], Loss: 0.3547\n",
            "correct: 15715\n",
            "total: 17476\n",
            "Accuracy on Epoch[19/25]: 89%\n",
            "Epoch [20/25], Step [100/1747], Loss: 0.5715\n",
            "Epoch [20/25], Step [200/1747], Loss: 0.0145\n",
            "Epoch [20/25], Step [300/1747], Loss: 0.1293\n",
            "Epoch [20/25], Step [400/1747], Loss: 0.1342\n",
            "Epoch [20/25], Step [500/1747], Loss: 0.0269\n",
            "Epoch [20/25], Step [600/1747], Loss: 0.0787\n",
            "Epoch [20/25], Step [700/1747], Loss: 0.0474\n",
            "Epoch [20/25], Step [800/1747], Loss: 0.0867\n",
            "Epoch [20/25], Step [900/1747], Loss: 0.0546\n",
            "Epoch [20/25], Step [1000/1747], Loss: 0.5698\n",
            "Epoch [20/25], Step [1100/1747], Loss: 0.0148\n",
            "Epoch [20/25], Step [1200/1747], Loss: 0.3543\n",
            "Epoch [20/25], Step [1300/1747], Loss: 0.1104\n",
            "Epoch [20/25], Step [1400/1747], Loss: 0.0753\n",
            "Epoch [20/25], Step [1500/1747], Loss: 0.3535\n",
            "Epoch [20/25], Step [1600/1747], Loss: 0.0569\n",
            "Epoch [20/25], Step [1700/1747], Loss: 0.4940\n",
            "correct: 15648\n",
            "total: 17476\n",
            "Accuracy on Epoch[20/25]: 89%\n",
            "Epoch [21/25], Step [100/1747], Loss: 0.5538\n",
            "Epoch [21/25], Step [200/1747], Loss: 0.0251\n",
            "Epoch [21/25], Step [300/1747], Loss: 0.2606\n",
            "Epoch [21/25], Step [400/1747], Loss: 0.2246\n",
            "Epoch [21/25], Step [500/1747], Loss: 0.0283\n",
            "Epoch [21/25], Step [600/1747], Loss: 0.0487\n",
            "Epoch [21/25], Step [700/1747], Loss: 0.0548\n",
            "Epoch [21/25], Step [800/1747], Loss: 0.0662\n",
            "Epoch [21/25], Step [900/1747], Loss: 0.0602\n",
            "Epoch [21/25], Step [1000/1747], Loss: 0.6127\n",
            "Epoch [21/25], Step [1100/1747], Loss: 0.0173\n",
            "Epoch [21/25], Step [1200/1747], Loss: 0.3682\n",
            "Epoch [21/25], Step [1300/1747], Loss: 0.1546\n",
            "Epoch [21/25], Step [1400/1747], Loss: 0.0484\n",
            "Epoch [21/25], Step [1500/1747], Loss: 0.4107\n",
            "Epoch [21/25], Step [1600/1747], Loss: 0.0877\n",
            "Epoch [21/25], Step [1700/1747], Loss: 0.4482\n",
            "correct: 15693\n",
            "total: 17476\n",
            "Accuracy on Epoch[21/25]: 89%\n",
            "Epoch [22/25], Step [100/1747], Loss: 0.5477\n",
            "Epoch [22/25], Step [200/1747], Loss: 0.0317\n",
            "Epoch [22/25], Step [300/1747], Loss: 0.1704\n",
            "Epoch [22/25], Step [400/1747], Loss: 0.2225\n",
            "Epoch [22/25], Step [500/1747], Loss: 0.0257\n",
            "Epoch [22/25], Step [600/1747], Loss: 0.0433\n",
            "Epoch [22/25], Step [700/1747], Loss: 0.0527\n",
            "Epoch [22/25], Step [800/1747], Loss: 0.0658\n",
            "Epoch [22/25], Step [900/1747], Loss: 0.0713\n",
            "Epoch [22/25], Step [1000/1747], Loss: 0.4515\n",
            "Epoch [22/25], Step [1100/1747], Loss: 0.0219\n",
            "Epoch [22/25], Step [1200/1747], Loss: 0.2736\n",
            "Epoch [22/25], Step [1300/1747], Loss: 0.1140\n",
            "Epoch [22/25], Step [1400/1747], Loss: 0.0566\n",
            "Epoch [22/25], Step [1500/1747], Loss: 0.3275\n",
            "Epoch [22/25], Step [1600/1747], Loss: 0.0274\n",
            "Epoch [22/25], Step [1700/1747], Loss: 0.3234\n",
            "correct: 15731\n",
            "total: 17476\n",
            "Accuracy on Epoch[22/25]: 90%\n",
            "Epoch [23/25], Step [100/1747], Loss: 0.4662\n",
            "Epoch [23/25], Step [200/1747], Loss: 0.0173\n",
            "Epoch [23/25], Step [300/1747], Loss: 0.1927\n",
            "Epoch [23/25], Step [400/1747], Loss: 0.2138\n",
            "Epoch [23/25], Step [500/1747], Loss: 0.0324\n",
            "Epoch [23/25], Step [600/1747], Loss: 0.0638\n",
            "Epoch [23/25], Step [700/1747], Loss: 0.0913\n",
            "Epoch [23/25], Step [800/1747], Loss: 0.0724\n",
            "Epoch [23/25], Step [900/1747], Loss: 0.0688\n",
            "Epoch [23/25], Step [1000/1747], Loss: 0.5719\n",
            "Epoch [23/25], Step [1100/1747], Loss: 0.0237\n",
            "Epoch [23/25], Step [1200/1747], Loss: 0.3189\n",
            "Epoch [23/25], Step [1300/1747], Loss: 0.1386\n",
            "Epoch [23/25], Step [1400/1747], Loss: 0.0364\n",
            "Epoch [23/25], Step [1500/1747], Loss: 0.2950\n",
            "Epoch [23/25], Step [1600/1747], Loss: 0.0579\n",
            "Epoch [23/25], Step [1700/1747], Loss: 0.3647\n",
            "correct: 15671\n",
            "total: 17476\n",
            "Accuracy on Epoch[23/25]: 89%\n",
            "Epoch [24/25], Step [100/1747], Loss: 0.5645\n",
            "Epoch [24/25], Step [200/1747], Loss: 0.0568\n",
            "Epoch [24/25], Step [300/1747], Loss: 0.1465\n",
            "Epoch [24/25], Step [400/1747], Loss: 0.1778\n",
            "Epoch [24/25], Step [500/1747], Loss: 0.0312\n",
            "Epoch [24/25], Step [600/1747], Loss: 0.0613\n",
            "Epoch [24/25], Step [700/1747], Loss: 0.0690\n",
            "Epoch [24/25], Step [800/1747], Loss: 0.0754\n",
            "Epoch [24/25], Step [900/1747], Loss: 0.0582\n",
            "Epoch [24/25], Step [1000/1747], Loss: 0.6486\n",
            "Epoch [24/25], Step [1100/1747], Loss: 0.0867\n",
            "Epoch [24/25], Step [1200/1747], Loss: 0.4430\n",
            "Epoch [24/25], Step [1300/1747], Loss: 0.1194\n",
            "Epoch [24/25], Step [1400/1747], Loss: 0.0768\n",
            "Epoch [24/25], Step [1500/1747], Loss: 0.6515\n",
            "Epoch [24/25], Step [1600/1747], Loss: 0.0352\n",
            "Epoch [24/25], Step [1700/1747], Loss: 0.3787\n",
            "correct: 15689\n",
            "total: 17476\n",
            "Accuracy on Epoch[24/25]: 89%\n",
            "Epoch [25/25], Step [100/1747], Loss: 0.5429\n",
            "Epoch [25/25], Step [200/1747], Loss: 0.0174\n",
            "Epoch [25/25], Step [300/1747], Loss: 0.2308\n",
            "Epoch [25/25], Step [400/1747], Loss: 0.1109\n",
            "Epoch [25/25], Step [500/1747], Loss: 0.0283\n",
            "Epoch [25/25], Step [600/1747], Loss: 0.0735\n",
            "Epoch [25/25], Step [700/1747], Loss: 0.0674\n",
            "Epoch [25/25], Step [800/1747], Loss: 0.0816\n",
            "Epoch [25/25], Step [900/1747], Loss: 0.0870\n",
            "Epoch [25/25], Step [1000/1747], Loss: 0.5839\n",
            "Epoch [25/25], Step [1100/1747], Loss: 0.0183\n",
            "Epoch [25/25], Step [1200/1747], Loss: 0.3432\n",
            "Epoch [25/25], Step [1300/1747], Loss: 0.1253\n",
            "Epoch [25/25], Step [1400/1747], Loss: 0.0672\n",
            "Epoch [25/25], Step [1500/1747], Loss: 0.4452\n",
            "Epoch [25/25], Step [1600/1747], Loss: 0.0312\n",
            "Epoch [25/25], Step [1700/1747], Loss: 0.3512\n",
            "correct: 15810\n",
            "total: 17476\n",
            "Accuracy on Epoch[25/25]: 90%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRbLYZRkXKDA",
        "colab_type": "code",
        "outputId": "256e0630-e07b-4bad-8591-5f2662cf31a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "all_target = None\n",
        "all_output = None\n",
        "for data, target in test_loader:\n",
        "  \n",
        "  outputs = baseline(data.to(device))\n",
        "  total += target.size(0)\n",
        "  correct += (F.log_softmax(outputs).data == target.to(device).data).sum()\n",
        "  if all_target is None:\n",
        "    all_target = target.data.numpy().reshape(target.data.shape[0],-1)\n",
        "    all_output = torch.round(torch.sigmoid(outputs)).cpu().data.numpy().reshape(target.data.shape[0],-1)\n",
        "  else:\n",
        "    all_target = np.concatenate((all_target,target.data.numpy().reshape(target.data.shape[0],-1)))\n",
        "    all_output = np.concatenate((all_output,torch.round(torch.sigmoid(outputs)).cpu().data.numpy().reshape(target.data.shape[0],-1)))\n",
        "print('Test Accuracy of the model on the test: %d %%' % (100 * correct / total))"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model on the test: 97 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqq8Dc0eIEvJ",
        "colab_type": "text"
      },
      "source": [
        "#### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI_o4lVaLRkr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "1ad4a95f-19a8-4b0f-a32c-57c593e4113c"
      },
      "source": [
        "all_output"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       ...,\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgUnUqokDOgU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "outputId": "f846af45-9a56-40c0-e41f-4a53aaf31c1c"
      },
      "source": [
        "cm = confusion_matrix(all_output, all_target)\n",
        "df_cm = pd.DataFrame(cm, range(2), range(2))\n",
        "# plt.figure(figsize=(10,7))\n",
        "sns.set(font_scale=1.4) # for label size\n",
        "sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEACAYAAAC08h1NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de1yUZd7H8Q+ioCEDsR4QjyG4iaGJ\nllohrodKgQyysjT1ydaUdHetLEtzy0zLUiJPm0VPru7aE22n9dS6HUBb7SDmptWmg0kinoUBT6Mw\nzx/3MkWogBwGvL7v12ter+a+r/ue32365eJ3XzPj5XK5XIiIiDEaeLoAERGpXQp+ERHDKPhFRAyj\n4BcRMYyCX0TEMAp+ERHDNPR0AWcOZ3m6BKljmoREe7oEqaPOOnOqdHxl8qZRs9AqvVZd5vHgFxGp\nNcVFnq6gTlDwi4g5XMWerqBOUPCLiDmKFfyg4BcRg7g04wcU/CJikqKznq6gTlDwi4g5dHMXUPCL\niEnU6gEU/CJiEt3cBRT8ImIQ3dy1KPhFxBya8QMKfhExSdEZT1dQJyj4RcQcavUACn4RMYlaPYCC\nX0RMohk/oOAXEZNoxg8o+EXEIK5i3dwFBb+ImEQzfkDBLyImUY8fUPCLiEn0IW2Agl9ETKIZP6Dg\nFxGTqMcPKPhFxCT6IhZAwS8iJtGMH1Dwi4hBXC7d3AUFv4iYRDN+QMEvIibRqh5AwS8iJtGMH1Dw\ni4hJtKoHUPCLiEnU6gEU/CJiErV6AAW/iJhEwQ8o+EXEJGr1AAp+ETGJbu4CCn4RMYlaPYCCX0RM\nolYPoOAXEZNoxg8o+EXEJAp+QMEvIiZxuTxdQZ2g4BcRc5zVqh5Q8IuISXRzF1Dwi4hJ1OMHFPwi\nYhL1+AEFv4iYRDN+QMEvIiZR8AMKfhExiKtIX7YOCn4RMYlm/ICCv0L+8fEG1qxPZ8d3Ozl6LI9W\nLZszIOZ6xo26Ez+/y2qtjrfeX8uylW+zN3c/rYNbcs+dCdyZEFtqTPKS18jY9AX7DxzC6TxDSHAL\nhgzqx5i7b6NJ48a1VqtcvMTEWIbfOZQeUd1o0eJXZP+4j3ffXcOcZxdQWHjc0+XVbzW0nHPPnj2k\npqaybds2du7cSWhoKKtWrSo1ZurUqbzzzjtljk1JSeHmm28utS01NZW//OUvHD58mLCwMKZMmUKf\nPn1KjSksLGTu3Ll88MEHOJ1OevXqxfTp02nTpk259Sr4K+D1v/6NVsEt+P39o2nZohnffW9n8Wt/\n4YvMbax4eT4NGjSo8Rreen8tT81dwH333EGfnt3ZvOUrZs1bhAsXwxPi3OMKj58gYciNdGjXGh+f\nRnz19bcs/fMbfPOfXSx47o81XqdU3UOTx5P9Yw7TZzxLzt5crr76KmY88SD9Yq7nhr634NLKlItX\nXDN/djt37iQ9PZ1u3bpRXFx83v9Hbdu25YUXXii1rUOHDqWep6amkpyczOTJk4mIiCAtLY1x48aR\nlpbGlVde6R730EMPsWPHDp544gmaNm3KSy+9xJgxY/j73/9OkyZNLlivgr8CFs59kqDLA93Pr+ne\nFZvNn2mz5vHF1n/Tq8fVVTr/jbeNZuiQQTwwduQ59589W8RLLy8j/qb+/P7+MQBc26MbBw8fYeEr\ny7kt/mYaNbT+Vz7x8MRSx/bu2Z2Tp06TuuJNjuXlc3lgQJVqlZo3NGE0hw8fdT/P2LCZo8fyeP21\nFPrFXMfHn3zqwerquRpq9fTv35+BAwcC1sx++/bt5xzXuHFjrr76/HnhdDpZsmQJo0aNYuzYsQBc\ne+21xMfHs2TJElJSUgDYtm0bn3zyCUuXLiUmJgaATp06MWjQIN5++21GjBhxwXprfqp6Cfh56Je4\nqnMnAA4cOuLetnfffh598jmiY++ke794bhv9AP9Mr/o/0m3bv+VoXj5xN/Uvtf2WmwaQl+9g67Yd\nFzw+MMAfAG9v7yrXIjXv56Ff4ssvvwIgpHVwbZdzaSkqqvijEqrrt/7MzEwKCgqIjf2phevt7c3g\nwYPJyMhw/yaRnp6Ov78/0dHR7nEhISFERUWRkZFR7utUaMZvt9vJyMggKyuL/Px8AAICAggNDaVv\n37507NixUhd3Kfhy69cAhLZvC0DugUPc/ds/EHR5II/87n6CAgNY+2E6k6c9w0tzZvCb6N4X/Vq7\ndu8BIDy0Q6ntHUPbA2D/IZtre3Qrte/s2SKcTifbdnzHn994h4S4G7H5N73oGsSz+kZb/d3vvt3p\n4UrquUrM+B0OBw6Ho8x2m82GzWa7qJfPzs6mZ8+enDx5kvDwcMaNG8eQIUPc++12O0CZTA0LC+PE\niRMcOHCA4OBg7HY7oaGhZX7ghIWFsXHjxnLruGDwnzp1imnTprFmzRoaNWpEu3bt3BeclZXFe++9\nx9y5cxkyZAizZ8/G19e3Yldfzx04dJhFry6nd8/u7pn/4tdW4MLF64vmEhhg/Rld36sH+w8cZuGr\ny93B73K5KCoq+5fPVVzM2bM/zTK8vH6aoec7CgDKBHeAv3+p/SV2Zv1Awj0T3M9vuXkATz7yuypd\ns3hOSEgwT/7xYf75zwy2ZP7b0+XUb5Xo8S9btoyFCxeW2T5x4kQmTZpU6Zfu3LkzkZGRhIWFUVBQ\nwFtvvcXkyZM5deoUiYmJgPXDxsfHh8a/WIgREGC1aPPy8ggODsbhcOD/33//P2ez2dyT8wu5YPC/\n8MILfPrppzz//PPceOON+Pj4lNrvdDpZv349s2bN4vnnn2f69OnlvmB9d+LESSY9OhNvb29mTZvs\n3v7p5i1E976Gpn5+pQL8+l5RzFuUSuHx4zT18+OLrV9z76RHy5z3T6+v5E+vr3Q/79k9ktcXzr2o\nGtu1DuGNV1M4eeoUX339La8u/z+Kiop47smyryt1m5/fZbz9t9c4e/YsY3/7oKfLqf8qsapn9OjR\nJCQklNl+sbP90aNHl3o+cOBARo0axYIFC9zBX1suGPyrV6/mscceIy4u7pz7fXx8iI2N5cyZMzz3\n3HOXfPCfOn2aBx55kr37cnl90VyCWzR37zt6LI/3133I++s+POexefkFNPXzo8uvw3jj1ZRS+yY9\n+hQx11/LsFsGu7f5XfbTMlGbzZrpOwoKae4b5N6eX2DN9ANspX/y+/r6uH8TuaZ7V5r/Kojps+dz\n97Bb6HZV54u5dPGAxo0b8947ywi9oh39Bw4jJyfX0yXVf5WY8VelpVNRN998M0899RRHjx4lKCgI\nm82G0+nk9OnTpTooJbP4wMBAd225uWX/PjgcDvdvBxdSbqunWbNm5Z6kWbNmnDp1qtxx9dmZs2eZ\nPO0Zdny3k1defIZOHa8otT8gwEaPbl24d8Tt5zy+RTMrsP38LnOHcolGjRrSvNmvymwvEXaF1cvf\ntXsPzZv9FPz23dkAdOzQ7oK1d+kcDkB2Tq6Cv55o2LAhb76xlB49unLz4LvYvv07T5d0SXDV8Tdw\nlfT27XY7ERER7u12ux0/Pz9atmzpHvevf/0Ll8uFl5eXe9yuXbsIDQ0t93UueCs6KiqKRYsWXbBn\nlJ+fz+LFi+nZs2e5L1ZfFRcXM/WpuXy+ZRsvPfvEOcPzhl49+H7XbsJC23NV505lHr9sk1VGt6s6\nc3mgjdX/+LjU9lUffESAzZ/uXSPOc6Sl5EZ029atLroGqT1eXl4s//NCfvOb67ht2Fg++zzT0yVd\nOmpoVc/FcLlcrF27ltatWxMUZE3ooqKi8Pf3Z82aNT8ruYi1a9cSHR3tDvmYmBgcDgcbNmxwj8vN\nzSUzM5O+ffuW+9oXnPHPmDGDe+65h379+tGnTx/CwsLcNxQKCgqw2+1s2rQJm83GsmXLKn/l9cSs\neYv44KMNjBs9nCaNG7Nt+7fufS1bNCO4RXMm3ncPw3/7B0YnTeHu2+IJadUSR0Ehu7L28OO+XGY9\nfvH92UYNGzLxvlHMmreIFs1/Re+e3fl8y1e8s/ofPD55Ao0aNQLgP7t288LCV7ipfzRtQoJxOs+w\nZdt2Vrz5HtG9e3K1Zvv1woKXZnP7sHhmz0nh+PET9Lo2yr1vb06uWj5VUUNv4Dp58iTp6ekA5OTk\nUFhYyLp16wCIjIwErPX9sbGxtG/fHofDQVpaGp9//jlz5/50L8/Hx4cJEyaQnJxMUFCQ+w1c2dnZ\nzJs3zz2uW7du9OvXj2nTpjF16lSaNm1KSkoKrVq1qtD9Ai9XOW8DLCgoYOXKlWzYsAG73e5e3mSz\n2ejYsSN9+/Zl+PDh57zDXBFnDmdd1HG16cbbRrNv/8Fz7ptw7wj3G6/2HzzE4tS/sHHzlxzNyycw\nwJ+wKzowdMhA4n+xBv+X57/QG7hKvPnuGpa98Tb79h+gVcsWjLozgeGJP91/OXz0GHNfWsq27d9y\n+MgxGjf2pU1IMLcOGcRt8TdV6beO2tQkJLr8QZewXd9vpkOHtufcN/Ppecx8en4tV1R3nHXmVOn4\n40/eVeGxfk+uLH/Qf+3du5cBAwacc9+cOXPo378/jz32GN988w1HjhyhUaNGREREMHbsWPr3L5sN\nqamprFixgsOHDxMeHn7Bj2xYt25dqY9saNv23H93fq7c4K9p9SH4pXaZHvxyflUO/hnDKzzWb+Yb\nVXqtukwf2SAi5tB37gIKfhExSQ31+OsbBb+IGMN1tuZX69QHCn4RMYdm/ICCX0RMoh4/oOAXEZNo\nxg8o+EXEIC4FP6DgFxGT6OYuoOAXEZNoxg8o+EXEJAp+QMEvIgbx8CfU1BkKfhExh2b8gIJfREyi\n4AcU/CJiENdZvYELFPwiYhLlPqDgFxGD6A1cFgW/iJhDwQ8o+EXEJGr1AAp+ETGIWj0WBb+IGMN1\nVsEPCn4RMYlaPYCCX0QMou9hsSj4RcQcCn5AwS8iBtGM36LgFxFjuM56uoK6QcEvIsbQjN+i4BcR\nYyj4LQp+ETGHy8vTFdQJCn4RMYZm/BYFv4gYw1WsGT8o+EXEIMVFCn5Q8IuIQdTqsSj4RcQYavVY\nFPwiYgyXPpwTUPCLiEE047co+EXEGLq5a1Hwi4gxNOO3KPhFxBguvXMXUPCLiEG0nNOi4BcRYxRr\nxg8o+EXEIGr1WBT8ImIMreqxNPB0ASIitcVV7FXhR2Xs2bOHGTNmMHToUCIiIoiLizvnuPT0dBIS\nEoiMjGTgwIEsX778nONSU1Pp378/Xbt2JTExkU2bNpUZU1hYyIwZM+jVqxfdu3dn/Pjx7N27t0L1\nKvhFxBjFLq8KPypj586dpKen0759ezp27HjOMVu3biUpKYnOnTvzyiuvkJiYyOzZs1m5cmWpcamp\nqSQnJzNixAhefvllOnTowLhx4/juu+9KjXvooYf46KOPeOKJJ0hOTubgwYOMGTOGkydPlluvl8vl\n2Tcxnzmc5cmXlzqoSUi0p0uQOuqsM6dKx399RXyFx0bu/nuFxxYXF9OggTWPnjp1Ktu3b2fVqlWl\nxtx3333k5+eTlpbm3vbEE0/w8ccfk5GRQYMGDXA6nVx33XXccccdPPLIIwAUFRURHx9PeHg4KSkp\nAGzbto077riDpUuXEhMTA8C+ffsYNGgQjz/+OCNGjLhgvZrxi4gxXK6KPyqjJPTPx+l0snnzZoYM\nGVJqe1xcHIcOHWLHjh0AZGZmUlBQQGxsrHuMt7c3gwcPJiMjg5J5enp6Ov7+/kRH/zRJCgkJISoq\nioyMjHLr1c1dETFGZVo4DocDh8NRZrvNZsNms1XqdbOzszlz5kyZNlB4eDgAWVlZREZGYrfbAcqM\nCwsL48SJExw4cIDg4GDsdjuhoaFlfuCEhYWxcePGcutR8IuIMYorcdN22bJlLFy4sMz2iRMnMmnS\npEq9bn5+PkCZHxglz0v2OxwOfHx8aNy4calxAQEBAOTl5REcHIzD4cDf37/M69hsNve5LsTjwR8Z\ncaenS5A6xrdhI0+XIJeoysz4R48eTUJCQpntlZ3t10UeD34RkdpSmTdwXUxL53xKZuy/bB2VPC/Z\nb7PZcDqdnD59Gl9fX/e4kll8YGCge1xubm6Z13E4HO5zXYhu7oqIMWpqOWd52rVrR6NGjcjKKr2K\ncdeuXQCEhoYCP/X2S3r9Jex2O35+frRs2dI9bvfu3fxyUeauXbvc57oQBb+IGMNViUd18vHxoXfv\n3qxdu7bU9lWrVtG8eXO6dOkCQFRUFP7+/qxZs8Y9pqioiLVr1xIdHY2Xl/UDKSYmBofDwYYNG9zj\ncnNzyczMpG/fvuXWo1aPiBijqLhm5ronT54kPT0dgJycHAoLC1m3bh0AkZGRtG7dmgceeICRI0cy\nffp04uPjyczMJC0tjRkzZrhX5/j4+DBhwgSSk5MJCgoiIiKCtLQ0srOzmTdvnvv1unXrRr9+/Zg2\nbRpTp06ladOmpKSk0KpVKxITE8ut1+Nv4LqyxTWefHmpg34sPOTpEqSOOn7ihyodvyF4WIXHRu9/\nq8Jj9+7dy4ABA865b86cOe4wTk9PZ/78+djtdlq0aMGYMWMYNWpUmWNSU1NZsWIFhw8fJjw8nClT\nptCnT59SYwoLC5k7dy7r1q3D6XTSq1cvpk+fTtu2bcutV8EvdY6CX86nqsGfEXx7hcf23Z9W/qB6\nSq0eETFGsUenuXWHgl9EjFGMPpYZFPwiYhCXgh9Q8IuIQYoU/ICCX0QMou9atyj4RcQYCn6Lgl9E\njKEev0XBLyLGqORX6V6yFPwiYgwt57Qo+EXEGEWeLqCOUPCLiDGKvTTjBwW/iBhEn9hgUfCLiDG0\nnNOi4BcRY2hVj0XBLyLG0Ec2WBT8ImIMzfgtCn4RMYZ6/BYFv4gYQ6t6LAp+ETGGWj0WBb+IGEOt\nHouCX0SMUaQZP6DgFxGDaMZvUfCLiDEU/BYFv4gYQ6t6LAp+ETGGVvVYFPwiYgy1eiwKfhExhr6I\nxaLgFxFjqNVjUfCLiDHU6rEo+EXEGFrVY1Hwi4gxihX9gIJfRAyim7sWBb+IGEM9fouCX0SMoVU9\nFgW/iBhDPX6Lgl9EjKHYtyj4RcQY6vFbFPwiYowizfkBBb+IGEQzfouCX0SMoZu7FgW/iBhDsW9p\n4OkC6oKb4vrz0mvP8eGW9/lqzwbW/ustHpz2AH5+l5V77OTHk0h9cwGbv1vPdwe/IOHOuFqouKyw\nX4eS+uYCtuxOZ/N365mdMoOAQFupMVW5TqkZ0dG9OX7ihzKPnH3/LjUuMNDGosXPsic7k4OHvmHV\nqhV06fJrD1VdfxVX4nEpU/AD/5M0kqKiIpJnL+a3w3/Pytf/xvAxt5GathAvrwu/42PkfXfg29iX\nT9ZvrKVqy2rRshl/fudP+Db25ff3TmXm1Ln06Xstf/pLcqn6q3KdUrMeevCP9ItJcD/i4kaU2p/2\nViqDBsXw8ENPMuLuCTRs1Ig1a1cS0jrYQxXXT0W4KvyojLfffptf//rXZR4zZ84sNS49PZ2EhAQi\nIyMZOHAgy5cvP+f5UlNT6d+/P127diUxMZFNmzZd9DWfi1o9wIR7HuTYkTz38y82ZZKfl89zC5/i\n2ut78NnGL897bM+Ov8HlctHuijbcWgOz/YlTfkvCnXEM6Dn0vGPufeAeGjZqyISRD1LgKATg4P7D\nrHh/KQOH9GP96o+Bql2n1Kz//GcXX3yx9Zz7YuMGcd111zD45rvIyLAC4LPPMtnxzQYmT76fKQ8/\nVZul1ms13eN/9dVX8ff3dz9v1qyZ+7+3bt1KUlISQ4cO5dFHHyUzM5PZs2fTsGFD7rrrLve41NRU\nkpOTmTx5MhEREaSlpTFu3DjS0tK48sorq6VOzfihVBiW+HrrNwC0bNX8gse6XBX7i+Tt7c24341h\nzadp/PvHT8n49xoefeoP+Pj6VL7gX+h/c1/S//mpO/QBvty8lZwfcxlwc1/3tqpcp3hObOxA9u3b\n7w59AIejgLVrPiQubpAHK6t/XJV4XIwuXbpw9dVXux9t2rRx71u0aBERERHMnj2b3r17k5SUxLBh\nw1i0aBHFxVZzyel0smTJEkaNGsXYsWPp06cPzz//PG3btmXJkiUXfd2/VG3Bv2/fPt59993qOp3H\nXXNdFABZ3/9QLeebu3gm4yffy6q3P+D+EZNZmvI6t919Cy8sebpK5/Vt7EubdiHs/NZeZt+u/2TR\nsVPoBY+v7uuUi/Paay/iKLCT/eNW/vd/U2jTJsS9r3PnTnyz4/syx3z77fe0a9dG92gqoRhXhR/V\nyel0snnzZoYMGVJqe1xcHIcOHWLHjh0AZGZmUlBQQGxsrHuMt7c3gwcPJiMjo8ITzfJUW6vn66+/\n5rHHHuPWW2+trlN6TIvg5vzukfv5NP0ztm/7tsrn69HramITbuTRiX/kvTfXALAp43Py8hy8sORp\nrryqE99tt/5hN2jQoFS/3auB9bPZ29u71DmLiqwPmA0I8KdBgwbk5zvKvG5+noMrwtqft67qvk6p\nPIejgJQXl7Jh42cUOArp1q0LU6Yk8fEnb3Ndn1gOHTrC5ZcHsGfP3jLHHj1m/QYXGBjA8eMnarv0\neqkyN20dDgcOR9l/VzabDZvNdo4jID4+nqNHj9KqVSsSExMZP348DRs2JDs7mzNnztCxY8dS48PD\nwwHIysoiMjISu92awP1yXFhYGCdOnODAgQMEB1f9vo56/L9wmV8TFv/5BYqKinj8dzPLP6ACovv3\nwXnayQd//7BUgH/6yWYArund3R386z9/h9btQsqcY0fu5lLPB/S4hZwfcy+6ppq4Tqm8bdt2sG3b\nDvfzjRs/49NPPyM94z0mJI1h5lPzPFjdpcdViZn8smXLWLhwYZntEydOZNKkSaW2NW/enEmTJtG1\na1e8vb3JyMhg8eLF7N27l2effZb8/HyAMj8wSp6X7Hc4HPj4+NC4ceNS4wICAgDIy8urneCPj4+v\n0ImOHz9e5WI8zbexL0uWz6dN+9aMuvV+DuQerJbzBjULwsfXh6/2nHvlT2BQgPu/J9zzID4+P/X9\n77gngX433kDSPQ+VOubg/kOANWMsLi4mIKDsDCQg0Eb+sbIzlpq6TqkeX321g507d9MjqhsAeXkO\nLg8MKDMu6PLA/+7Pr9X66rPKrNYZPXo0CQkJZbafa7YfHR1NdHS0+/n111+Pv78/CxYsICkp6eKK\nrUHlBn9WVhZhYWFERERccFxOTg65uRc/A/W0hg29SUl9lquu7sy9t0/k+3P0zC9W3rF8Tp08xchb\nxp1zf0mIA2Vet9+NN3DGeea8rZhTJ0+T82MuYVeW7eWHdbqCLzZlltpWk9cp1atkdvrtt98zYEB0\nmf1XXhlOdvZetXkqoTKtngu1dCpi8ODBLFiwgB07drhbOr9sHZU8L5nR22w2nE4np0+fxtfX1z2u\n5DeCwMDAi67n58oN/vDwcNq3b8+cOXMuOO6DDz7giy++qJaiapuXlxfPL3ma3jf0ZPzIB9m2ZXu1\nnn/jR/9i3O9G09TWlM0bqv/P6KN1Gdx6ZyxN/f0oLLB+84rq1Y3W7UJ47skX3eNq+jqlenSPiqRT\np1Defde6H7R69T8ZNeoObrihFxs3fgaAv39TBg8ZwJtvvufJUuud4mq6OVpZ7dq1o1GjRmRlZdG3\n708r7Xbt2gVAaKg1cSvp7dvt9lKTbbvdjp+fHy1btqyWesoN/q5du7Jhw4YKnay67jjXthnPPcLg\noYNYMj+VkydO0q3HVe59+/cd5EDuQULaBPOPz99h8bxUFs971b3/mj5RBP0qkGYtfgXAVVd35sR/\nZ2AfrPoIgM//lcmqv60jJfVZXv/TX/l66w6Ki4tp3TaEvgOvY97MhfyQlX3R9acuWs4twwazZPl8\nXk55HX9bUx6eMYmvvvya9as/qdR1Su167bUX+WHPj3z11Xby8xx069aFhx5OYt++/SxZ/DoAq1et\nZ/PmLaS+lsy0x+eQl5fPQw8n4eXlRfL8lz17AfVMbSbU6tWr8fLy4qqrrsLHx4fevXuzdu1axowZ\n4x6zatUqmjdvTpcuXQCIiorC39+fNWvWuIO/qKiItWvXEh0dXW1vtCw3+O+77z5iYmLKPVFMTAwf\nfvhhtRRV2/r2vw6ACQ+OZcKDY0vtW/j8UhY+/wpeXl40bNiQBg1K/8FPemQc117fw/18xNg7GDH2\nDgCubHGNe/uUpBmMvO9Obrs7nvF/+B+czjPk/LiPjR9v5vChI1Wq/+D+Q4xOHM+jMyfz0mvPcebM\nGT5al8Fzf3yx1A/jilyn1K5vvvme22+PZ/z40Vx2WRMOHDjE+++v45lZyRw5cgywJlTDbruX2XOm\nkfzi0zRu7Mvnn2UyZPBd5OTU3/aqJ9TUG7jGjh1Lr1696NSpE15eXmzYsIG//vWvDBs2jLZt2wLw\nwAMPMHLkSKZPn058fDyZmZmkpaUxY8YMGvx39Z6Pjw8TJkwgOTmZoKAg9xu4srOzmTev+m70e7k8\nPE3/eTiKAPxYeKj8QWKk4yd+qNLxd7Wv+HLzlXsq/r6kZ555hoyMDA4cOMDZs2fp0KEDiYmJjB49\nutRKvvT0dObPn4/dbqdFixaMGTOGUaNGlTlfamoqK1as4PDhw4SHhzNlyhT69OlT4XrKo+CXOkfB\nL+dT1eC/vf35P/rkl9L2XLr3T7SOX0SMUZl1/JcyBb+IGONS/7jlilLwi4gx6uvKw+qm4BcRY+ir\nFy0KfhExRmW/YOVSpeAXEWNoxm9R8IuIMdTjtyj4RcQYWtVjUfCLiDG0jt+i4BcRY6jHb1Hwi4gx\nilxq9oCCX0QMolaPRcEvIsbw1Bex1DUKfhExhmLfouAXEWPo5q5FwS8ixlDwWxT8ImIMreqxKPhF\nxBha1WNR8IuIMfRZPRYFv4gYQz1+i4JfRIyhGb9FwS8ixijS53MCCn4RMYjeuWtR8IuIMbSqx6Lg\nFxFjaMZvUfCLiDE047co+EXEGJrxWxT8ImIMfWSDRcEvIsZQq8ei4BcRY7g04wcU/CJiEH1kg0XB\nLyLG0Ec2WBT8ImIMzfgtCn4RMUZRsXr8oOAXEYNoVY9FwS8ixlCP36LgFxFjqMdvUfCLiDE047co\n+EXEGLq5a1Hwi4gx1OqxKPhFxBhq9VgU/CJiDH0ss0XBLyLG0Dp+i4JfRIyhGb9FwS8ixijWxzID\nCn4RMYhu7loU/CJiDAW/xQad4/QAAAI7SURBVMulPwkREaM08HQBIiJSuxT8IiKGUfCLiBhGwS8i\nYhgFv4iIYRT8IiKGUfCLiBhGwS8iYhgFv4iIYRT8HvbDDz8wduxYunfvTu/evXn66ac5efKkp8sS\nD9qzZw8zZsxg6NChREREEBcX5+mS5BKjz+rxIIfDwahRowgJCSElJYWjR48yZ84cjh49SnJysqfL\nEw/ZuXMn6enpdOvWjeLiYn2+jFQ7Bb8HvfHGGzgcDt59912CgoIA8Pb25uGHHyYpKYnw8HAPVyie\n0L9/fwYOHAjA1KlT2b59u4crkkuNWj0elJGRQe/evd2hD3DTTTfh4+NDRkaGBysTT2rQQP8spWbp\nb5gH2e12wsLCSm3z8fGhXbt2ZGVleagqEbnUKfg9yOFwYLPZymy32Wzk5+d7oCIRMYGCX0TEMAp+\nD7LZbDgcjjLbHQ4HAQEBHqhIREyg4Pegjh07YrfbS21zOp1kZ2cTGhrqoapE5FKn4Pegvn37snnz\nZo4dO+betn79epxOJzExMR6sTEQuZVrH70HDhw9nxYoVJCUlkZSUxJEjR3j22WcZMmRImdU+Yo6T\nJ0+Snp4OQE5ODoWFhaxbtw6AyMhIWrdu7cny5BKgL1v3sN27dzNr1iy2bNmCr68vsbGxTJkyhSZN\nmni6NPGQvXv3MmDAgHPumzNnDomJibVckVxqFPwiIoZRj19ExDAKfhERwyj4RUQMo+AXETGMgl9E\nxDAKfhERwyj4RUQMo+AXETGMgl9ExDD/D3NPQo385CpKAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFLSVy5JEjxM",
        "colab_type": "text"
      },
      "source": [
        "#### CLassification report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV_j87gAIPy2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "2dacfda2-af29-47df-d8d5-e33ffbfd7d77"
      },
      "source": [
        "print(classification_report(all_target, all_output))"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.90      0.95      2185\n",
            "         1.0       0.19      0.96      0.32        52\n",
            "\n",
            "    accuracy                           0.91      2237\n",
            "   macro avg       0.60      0.93      0.64      2237\n",
            "weighted avg       0.98      0.91      0.93      2237\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cLtErTXIZJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}